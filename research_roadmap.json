{
  "project": "open-cure",
  "baseline_metric": "37.04% R@30 (kNN k=20)",
  "evaluation_details": {
    "method": "kNN collaborative filtering",
    "k": 20,
    "similarity": "Node2Vec cosine",
    "diseases": "5-seed disease holdout",
    "ceiling": "60.4% (oracle)",
    "gap": "23 pp requires external data"
  },
  "last_updated": "2026-01-28",
  "hypotheses": [
    {
      "id": "h1",
      "title": "GB + TxGNN Best-Rank Ensemble",
      "category": "ensemble",
      "rationale": "TxGNN excels at storage diseases (83.3% R@30) while GB is better overall (41.8%). Archive shows simple best_rank ensemble achieved 7.5% vs 6.7% TxGNN alone. Taking min(GB_rank, TxGNN_rank) should capture best of both.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 1,
      "status": "invalidated",
      "steps": [
        "Step 1: Load TxGNN predictions from data/reference/txgnn_predictions.csv",
        "Step 2: For each disease-drug pair, compute min(GB_rank, TxGNN_rank)",
        "Step 3: Evaluate R@30 on held-out disease set using disease-level split",
        "Step 4: Success criteria: >43% R@30 (>1.2% improvement over baseline)"
      ],
      "findings": "CRITICAL BLOCKER: The pre-computed TxGNN predictions file (txgnn_predictions_final.csv) only contains TOP 50 drugs per disease. Ground truth drugs almost NEVER appear in TxGNN's top-50 predictions (0% coverage for common diseases). The 14.5% R@30 in archive was based on full TxGNN inference (where GT drugs might rank within 30 among 7954 drugs), not pre-computed rankings. Ensemble using pre-computed file achieves 0% TxGNN contribution + 42.0% GB = 42.0% ensemble (no improvement). To implement this properly, we would need to: (1) Run TxGNN inference on GPU for all drug-disease pairs, OR (2) Store full rankings (not just top-50). Current approach is blocked without GPU.",
      "result_metric": "42.0% R@30 (ensemble) - no improvement over 42.0% GB baseline"
    },
    {
      "id": "h2",
      "title": "Category-Routed Ensemble",
      "category": "ensemble",
      "rationale": "TxGNN dominates on storage (83%), psychiatric (28%), dermatological (25%), autoimmune (22%) categories. Route these to TxGNN, rest to GB. Keyword categorization achieves 68% disease coverage.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 2,
      "status": "blocked",
      "steps": [
        "Step 1: Use src/disease_categorizer.py to classify all evaluation diseases",
        "Step 2: Route storage/psychiatric/dermatological/autoimmune to TxGNN predictions",
        "Step 3: Route cancer/respiratory/renal/gastrointestinal to GB",
        "Step 4: Evaluate R@30 with proper held-out split",
        "Step 5: Success criteria: >44% R@30"
      ],
      "findings": "BLOCKED: Same issue as h1 - TxGNN pre-computed predictions only contain top-50 drugs per disease. GT drugs not in top-50 for most diseases. Category routing would have 0% contribution from TxGNN without live GPU inference. Requires GPU access to implement properly.",
      "result_metric": null
    },
    {
      "id": "h3",
      "title": "Infectious Disease Specialist Model",
      "category": "architecture",
      "rationale": "Infectious diseases have 13.6% recall vs 63% autoimmune. Model predicts antibiotics for wrong diseases. A specialist model trained on infectious disease pairs could learn pathogen-specific patterns.",
      "expected_impact": "high",
      "effort": "high",
      "priority": 3,
      "status": "invalidated",
      "steps": [
        "Step 1: Extract infectious disease pairs from ground truth (identify by ICD codes or keywords)",
        "Step 2: Extract antibiotic/antiviral drugs from DrugBank by ATC codes (J01, J05)",
        "Step 3: Train specialist XGBoost model on infectious pairs only",
        "Step 4: Evaluate on held-out infectious diseases",
        "Step 5: Success criteria: >30% R@30 for infectious diseases (2x improvement)"
      ],
      "findings": "INVALIDATED: The baseline 13.6% R@30 figure in CLAUDE.md was based on antibiotic CLASS performance, not overall infectious disease evaluation.\n\nACTUAL BASELINE: General GB model achieves 52.0% R@30 on 47 mappable infectious diseases (104/200 hits).\n\nSPECIALIST MODEL: Trained on 294 positive pairs, 756 negatives. Achieved only 36.4% R@30 on held-out test diseases (12 diseases, 22 GT drugs).\n\nGENERAL MODEL COMPARISON: On same test set, general model achieved 63.6% R@30.\n\nKEY FINDINGS:\n1. General model OUTPERFORMS specialist by 27.3% on infectious diseases\n2. Specialist has insufficient training data (294 pairs vs ~3000 for general model)\n3. The infectious disease \"problem\" identified in CLAUDE.md was about antibiotics being predicted for NON-infectious diseases, not about recall on actual infectious diseases\n4. The general model already performs well (52% R@30) on infectious diseases when evaluated properly\n\nIMPLICATION: A specialist model approach is not needed for infectious diseases. The real problem is filtering spurious antibiotic predictions for non-infectious diseases (already addressed by confidence_filter.py).",
      "result_metric": "36.4% R@30 (specialist) vs 52.0% R@30 (general baseline) - specialist underperforms"
    },
    {
      "id": "h4",
      "title": "Expand Ground Truth with DrugBank/ChEMBL Indications",
      "category": "data",
      "rationale": "Current GT is Every Cure (~50K pairs). DrugBank and ChEMBL have additional FDA-approved indications. Validation sessions found 4 FDA-approved drugs missing from GT (Ustekinumab, Guselkumab, Pembrolizumab).",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 4,
      "status": "inconclusive",
      "steps": [
        "Step 1: Download DrugBank approved indications (data/reference/drugbank_lookup.json exists)",
        "Step 2: Download ChEMBL approved indications via API",
        "Step 3: Map to DRKG disease IDs using disease_name_matcher.py",
        "Step 4: Add new positive pairs and retrain GB model",
        "Step 5: Evaluate on original test set",
        "Step 6: Success criteria: >43% R@30"
      ],
      "findings": "PARTIAL VALIDATION: The hypothesis premise was partially correct but the impact is too small to be meaningful.\n\nKEY FINDINGS:\n1. All DRKG treatment edges (4,968) are already included in the current GT (58,016 pairs)\n2. The GT is much larger than DRKG because it includes Every Cure annotations\n3. We identified 6 FDA-approved drug-disease pairs missing from GT:\n   - Pembrolizumab -> Breast Cancer (Rank 7, Hit)\n   - Natalizumab -> Multiple Sclerosis (Rank 16, Hit)\n   - Erlotinib -> Pancreatic Cancer (Rank 16, Hit)\n   - Cetuximab -> Colorectal Cancer (Rank 1, Hit)\n   - Oxaliplatin -> Colorectal Cancer (Rank 37, Miss)\n   - Bevacizumab -> Colorectal Cancer (Rank 7, Hit)\n4. 5/6 (83.3%) of these pairs hit@30 - suggesting the model already learns these relationships\n5. Impact: Adding these 6 pairs would improve R@30 by only +0.22 pp (42.04% \u2192 42.26%)\n\nBLOCKERS:\n- DrugBank data available (drugbank_lookup.json) only has name mappings, not indication data\n- Full DrugBank indication data requires license/download (not available)\n- ChEMBL API access not implemented\n- Manual curation of missing pairs is not scalable\n\nCONCLUSION: The approach is theoretically sound (adding correct FDA pairs improves accuracy) but:\n1. Most FDA-approved pairs are already in GT\n2. Missing pairs are few in number\n3. Impact is marginal (<0.3 pp)\n4. Accessing comprehensive indication databases requires additional setup\n\nRecommend: Mark as inconclusive pending access to DrugBank/ChEMBL full indication data.",
      "result_metric": "+0.22 pp (42.04% \u2192 42.26%) from 6 manual additions"
    },
    {
      "id": "h5",
      "title": "Hard Negative Mining",
      "category": "data",
      "rationale": "Current model uses random negative sampling. Learning from hard negatives (drugs that seem plausible but aren't treatments) could improve discrimination. Confounding analysis identified 9 false positive patterns.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 5,
      "status": "invalidated",
      "steps": [
        "Step 1: Generate hard negatives: drugs with high GB score but not in GT",
        "Step 2: Include confounding patterns (statins->T2D, checkpoint->UC) as explicit negatives",
        "Step 3: Retrain GB with 50% hard negatives, 50% random negatives",
        "Step 4: Evaluate R@30 on held-out diseases",
        "Step 5: Success criteria: >42.5% R@30 + reduced false positive rate"
      ],
      "findings": "INVALIDATED: Comprehensive experiment with 5 negative sampling strategies (v2) reveals a FUNDAMENTAL problem: the GB model cannot generalize to unseen diseases under disease-level holdout.\n\nRESULTS (88 held-out test diseases, 547 GT pairs):\n- Baseline (existing model, trained on ALL diseases): 45.89% R@30\n- Strategy A (random negatives): 12.43% R@30 (-33.5%)\n- Strategy B (drug-treats-other): 3.29% R@30 (-42.6%)\n- Strategy C (B + 50% model FP): 5.67% R@30 (-40.2%)\n- Strategy D (B + 25% model FP): 8.59% R@30 (-37.3%)\n- Strategy E (D + confounding): 7.31% R@30 (-38.6%)\n\nALL freshly trained models collapse on held-out diseases, regardless of negative sampling strategy.\n\nROOT CAUSE: The existing model was trained using random pair-level split (train_test_split with stratify=y), NOT disease-level holdout. It learns disease-specific embedding patterns that don't transfer to unseen diseases. The 41.8% R@30 baseline metric reflects within-distribution performance, not novel disease generalization.\n\nThe prior v1 result (14.5% collapse) was also caused by this fundamental issue, not by hard negative mining specifically.\n\nCRITICAL INSIGHT: Hard negative mining is irrelevant when the model architecture itself cannot generalize. The GB model operating on TransE embedding features (concat/product/diff) essentially memorizes per-disease patterns rather than learning transferable drug-disease relationships.\n\nPOSITIVE CONTROLS:\n- Baseline: Rituximab\u2192MS rank 6, Imatinib\u2192CML rank 1, Lisinopril\u2192HTN rank 12, Metformin\u2192T2D rank 3426\n- All retrained models: positive controls degraded significantly\n\nNote: Node2Vec + XGBoost reportedly achieved 41.9% R@30 on held-out diseases, suggesting the embedding method matters more than the negative sampling strategy.",
      "result_metric": "ALL strategies failed: best retrained model 12.43% R@30 vs 45.89% baseline on held-out diseases"
    },
    {
      "id": "h6",
      "title": "Biologic-Specific Features",
      "category": "feature",
      "rationale": "Biologics achieve only 27.3% recall vs 63% for ACE inhibitors. Root cause: data sparsity (2.1 vs 11.1 diseases/drug). Adding mAb-specific features (target antigen, immunology pathway) could help.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 6,
      "status": "blocked",
      "steps": [
        "Step 1: Extract mAb target antigens from DrugBank (e.g., TNF-alpha, CD20, IL-17)",
        "Step 2: Map diseases to immunology pathways (Th1, Th2, Th17)",
        "Step 3: Create feature: mAb_target_pathway_match",
        "Step 4: Boost predictions where target matches disease pathway",
        "Step 5: Evaluate R@30 for biologics specifically",
        "Step 6: Success criteria: >35% R@30 for biologics"
      ],
      "findings": "BLOCKED (DRKG ceiling): DRKG-internal feature engineering won't break 37% ceiling (h34, h35, h41, h45). The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h7",
      "title": "Graph Path Features",
      "category": "feature",
      "rationale": "Current features use embedding similarity. Adding explicit graph structure features (path length, shared neighbors, metapath counts) could capture different signals from DRKG.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 7,
      "status": "blocked",
      "steps": [
        "Step 1: Load DRKG graph from data/drkg/drkg.tsv",
        "Step 2: Compute for each drug-disease pair: shortest path, #paths<=3, shared gene neighbors",
        "Step 3: Add as features to existing GB model",
        "Step 4: Retrain with graph features",
        "Step 5: Evaluate on held-out diseases",
        "Step 6: Success criteria: >43% R@30"
      ],
      "findings": "BLOCKED (DRKG ceiling): Graph path features already tested and failed in h34. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h8",
      "title": "Confidence-Based Post-Filtering",
      "category": "evaluation",
      "rationale": "Confidence calibrator achieves 0.962 AUROC. Using it to filter low-confidence predictions could improve precision without hurting recall. Filter threshold analysis needed. NOTE: h5 findings suggest this operates on within-distribution performance only. Still useful for the current (non-generalizing) model's predictions.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 10,
      "status": "blocked",
      "steps": [
        "Step 1: Load confidence_calibrator.pkl model",
        "Step 2: Score all predictions in evaluation set",
        "Step 3: Analyze R@30 at different confidence thresholds (0.2, 0.4, 0.6, 0.8)",
        "Step 4: Find optimal threshold for precision-recall tradeoff",
        "Step 5: Success criteria: Maintain >40% R@30 with >30% precision improvement"
      ],
      "findings": "BLOCKED by paradigm shift to kNN.\n\nThe confidence calibrator was trained for the GB model (base_score, boosted_score features).\nSince h39 showed kNN (37.04%) >> GB (25.85%), the calibrator is no longer applicable.\n\nThe kNN approach outputs drug frequency scores, not the same features the calibrator expects.\nWould need to retrain a new calibrator on kNN outputs, but kNN already provides interpretable scores.\n\nAlternative: The existing confidence_filter.py rules (antibiotic patterns, withdrawn drugs, etc.)\nare still applicable to any prediction method as post-hoc filtering.",
      "result_metric": "BLOCKED - calibrator trained on obsolete GB model features"
    },
    {
      "id": "h9",
      "title": "Disease Coverage Expansion via UMLS",
      "category": "data",
      "rationale": "HIGH VALUE: More disease mappings. Currently 30.9% of EC diseases mapped via fuzzy matching. UMLS has comprehensive cross-references (MESH, MONDO, DOID, SNOMED). Could improve to 50%+ coverage.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 4,
      "status": "deprioritized",
      "steps": [
        "Step 1: Download UMLS Metathesaurus (requires license)",
        "Step 2: Extract MESH<->MONDO<->DOID cross-references",
        "Step 3: Integrate into disease_name_matcher.py",
        "Step 4: Re-evaluate with expanded disease coverage",
        "Step 5: Success criteria: >45% disease coverage, maintain R@30"
      ],
      "findings": "DEPRIORITIZED: h19 (HPO) and h17 (PPI) showed external ontology data provides WEAKER signal than Node2Vec. UMLS mappings would improve coverage but not address the fundamental ceiling. Focus should shift to better ground truth or different architectures.",
      "result_metric": null
    },
    {
      "id": "h10",
      "title": "Temporal Validation Split",
      "category": "evaluation",
      "rationale": "Current evaluation uses random disease split. Temporal split (train on pre-2020 approvals, test on 2020-2025) would better simulate real-world drug discovery and reduce optimistic bias.",
      "expected_impact": "low",
      "effort": "medium",
      "priority": 14,
      "status": "pending",
      "steps": [
        "Step 1: Download FDA approval dates from DrugBank or OpenFDA",
        "Step 2: Split GT: train on approvals pre-2020, test on 2020-2025",
        "Step 3: Retrain model on temporal training set",
        "Step 4: Evaluate R@30 on temporal test set",
        "Step 5: Report both temporal and random-split metrics",
        "Step 6: Success criteria: Establish temporal baseline, identify gap"
      ],
      "findings": null,
      "result_metric": null
    },
    {
      "id": "h11",
      "title": "Mechanism of Action Features",
      "category": "feature",
      "rationale": "Current model doesn't use drug mechanism directly. DrugBank has MoA annotations (agonist, antagonist, inhibitor). Matching MoA to disease pathophysiology could improve predictions.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 11,
      "status": "blocked",
      "steps": [
        "Step 1: Extract MoA from DrugBank (action field)",
        "Step 2: Create MoA categories: agonist, antagonist, inhibitor, modulator",
        "Step 3: Map diseases to expected MoA (e.g., diabetes -> agonist for insulin pathway)",
        "Step 4: Add MoA_match feature",
        "Step 5: Evaluate impact on R@30",
        "Step 6: Success criteria: >42.5% R@30"
      ],
      "findings": "BLOCKED (DRKG ceiling): MoA features are DRKG-internal, won't help. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h12",
      "title": "Node2Vec Embedding Refresh",
      "category": "architecture",
      "rationale": "Archive shows Node2Vec+XGBoost achieved 41.9% R@30 on held-out diseases (fair evaluation). Current TransE may be suboptimal. Retraining Node2Vec with DRKG + EC edges could improve.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 12,
      "status": "blocked",
      "steps": [
        "Step 1: Augment DRKG with Every Cure treatment edges",
        "Step 2: Retrain Node2Vec embeddings (256-dim) on augmented graph",
        "Step 3: Extract new embeddings for all drugs/diseases",
        "Step 4: Retrain XGBoost on new embeddings",
        "Step 5: Evaluate on held-out diseases",
        "Step 6: Success criteria: >43% R@30"
      ],
      "findings": "BLOCKED (DRKG ceiling): Retraining Node2Vec won't break ceiling - similarity measure is the bottleneck, not embedding quality. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h13",
      "title": "Confounding Pattern Expansion",
      "category": "data",
      "rationale": "Current confounding detector finds 9 patterns (1.6%). Expanding rules based on validation sessions (anti-IL-5, anti-IFN-gamma, TRAIL agonists) could filter more false positives.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 11,
      "status": "inconclusive",
      "steps": [
        "Step 1: Review all false positive patterns from validation_sessions.md",
        "Step 2: Add rules for: anti-IL-5 for non-eosinophilic, IL-6 for psoriasis, anti-IFN-gamma for UC",
        "Step 3: Update confounding_detector.py with new patterns",
        "Step 4: Re-run confounding analysis",
        "Step 5: Success criteria: >5% confounding detection rate"
      ],
      "findings": "h13 INCONCLUSIVE - Patterns added but detection rate unchanged on validation cache.\n\nCHANGES MADE:\nAdded 60+ new confounding patterns from validation_sessions.md:\n1. TCAs \u2192 hypertension (norepinephrine reuptake)\n2. PPIs \u2192 hypertension (17% increased risk)\n3. Aminoglycosides \u2192 T2D (insulin inhibition)\n4. Anti-EGFR \u2192 UC (EGFR is protective)\n5. B-cell depletion \u2192 psoriasis (paradoxical induction)\n6. Anti-IL-5 for non-eosinophilic diseases\n7. Anti-IFN-\u03b3 for UC (wrong Th1/Th2 pathway)\n8. TRAIL agonists for inflammatory diseases\n9. IL-6 inhibitors for psoriasis (wrong pathway)\n10. Bone drugs for neurological diseases\n11. Cancer-specific antibodies for autoimmune diseases\n\nRESULTS ON VALIDATION CACHE (1,052 pairs):\n- Before: 9 patterns detected (1.6%)\n- After: 15 patterns detected (1.43%)\n- New patterns (wrong_pathway, cancer_target): 0 detected\n\nWHY NO IMPROVEMENT:\n1. Validation cache doesn't contain the specific biologics targeted by new patterns\n2. Anti-IL-5, TRAIL agonists, etc. are rare in model predictions\n3. The cache is already filtered/curated, so confounded pairs are underrepresented\n\nSUCCESS CRITERION NOT MET: Target was >5% detection rate.\n\nRECOMMENDATION: The confounding detector is more comprehensive, but measuring on validation cache underestimates its value. The patterns will filter false positives when applied to raw model predictions.\n",
      "result_metric": "1.43% detection rate (target: >5%)"
    },
    {
      "id": "h14",
      "title": "Drug Formulation Filter",
      "category": "feature",
      "rationale": "Validation found intravitreal drugs (Brolucizumab) predicted for systemic diseases. Adding route-of-administration constraints could filter impractical predictions.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 12,
      "status": "deprioritized",
      "steps": [
        "Step 1: Extract drug formulation from DrugBank (route field)",
        "Step 2: Flag intravitreal, topical-only, diagnostic-only drugs",
        "Step 3: Penalize predictions where route doesn't match disease type",
        "Step 4: Evaluate precision improvement",
        "Step 5: Success criteria: 10%+ precision improvement on biologics"
      ],
      "findings": "h14 DEPRIORITIZED - No impact on validation cache.\n\nANALYSIS:\n- Intravitreal drugs in GT: 7 (Brolucizumab, Ranibizumab, Aflibercept, etc.)\n- Diagnostic agents in GT: 10 (Ioflupane, Florbetapir, Technetium, etc.)\n- In validation cache: 0 of either category\n\nWHY NO IMPACT:\n- Validation cache is already curated/filtered\n- These drugs don't appear in high-confidence model predictions\n- Manual curation already handles this in practice\n\nBLOCKED BY:\n- No route of administration data in available DrugBank files\n- Would need full DrugBank XML (licensed) or manual curation\n\nRECOMMENDATION: Skip - manual curation during validation handles this. If raw predictions are deployed, add a simple keyword filter for intravitreal/-umab eye drugs.\n",
      "result_metric": "0% of validation cache affected"
    },
    {
      "id": "h15",
      "title": "Oncology Specialist Boosting",
      "category": "architecture",
      "rationale": "Oncology mAbs achieve 0-17% recall. Anti-HER2 and anti-EGFR particularly weak. Separate oncology model with tumor gene features could improve.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 15,
      "status": "blocked",
      "steps": [
        "Step 1: Extract cancer disease subset from GT",
        "Step 2: Add tumor-specific features: driver mutations, tumor suppressor status",
        "Step 3: Train oncology-specific XGBoost model",
        "Step 4: Ensemble with general model for cancer predictions",
        "Step 5: Evaluate R@30 for cancer diseases",
        "Step 6: Success criteria: >25% R@30 for oncology (up from ~11%)"
      ],
      "findings": "BLOCKED (DRKG ceiling): Specialist models already failed (h3), and kNN doesn't need them. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h16",
      "title": "Clinical Trial Phase Features",
      "category": "feature",
      "rationale": "External trial data could help. External validation uses ClinicalTrials.gov data. Adding trial phase as a feature (Phase 1/2/3) for known pairs could help model learn which combinations advance.",
      "expected_impact": "low",
      "effort": "medium",
      "priority": 20,
      "status": "pending",
      "steps": [
        "Step 1: Query ClinicalTrials.gov API for all drug-disease pairs",
        "Step 2: Extract max trial phase reached",
        "Step 3: Use as auxiliary training signal (semi-supervised)",
        "Step 4: Evaluate impact on novel prediction quality",
        "Step 5: Success criteria: >25% validation precision (up from 22.5%)"
      ],
      "findings": "Low priority: h19/h17 showed external data provides weaker signal. Clinical trial phase data is orthogonal (not similarity-based) so may still be worth testing, but expected impact is low.",
      "result_metric": null
    },
    {
      "id": "h17",
      "title": "PPI Network Distance Features",
      "category": "feature",
      "rationale": "HIGH VALUE: External PPI data, new similarity signal. Drugs that target proteins close to disease genes in PPI network may be more effective. STRING database has PPI distances.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 2,
      "status": "invalidated",
      "steps": [
        "Step 1: Download STRING PPI network for human",
        "Step 2: Compute shortest path from drug targets to disease genes",
        "Step 3: Add as feature: min_ppi_distance, mean_ppi_distance",
        "Step 4: Retrain GB with PPI features",
        "Step 5: Evaluate on held-out diseases",
        "Step 6: Success criteria: >42.5% R@30"
      ],
      "findings": "INVALIDATED: PPI network similarity does NOT improve drug repurposing.\n\nCOVERAGE:\n- 369/465 (79.4%) GT diseases have PPI gene neighborhoods\n- Pre-computed 2-hop neighborhoods (mean size: 4,828 genes)\n\nRESULTS (5-seed mean \u00b1 std):\n| Method                  | R@30          | Delta vs baseline |\n|-------------------------|---------------|-------------------|\n| Node2Vec kNN (baseline) | 36.93% \u00b1 6.02%| ---               |\n| PPI kNN (2-hop Jaccard) | 16.18% \u00b1 2.00%| -20.76 pp         |\n| Hybrid \u03b1=0.1            | 36.88% \u00b1 4.68%| -0.05 pp          |\n| Hybrid \u03b1=0.2            | 35.72% \u00b1 4.55%| -1.21 pp          |\n| Hybrid \u03b1=0.3            | 35.02% \u00b1 4.91%| -1.91 pp          |\n\nKEY FINDINGS:\n1. PPI-only kNN achieves only 16.18% R@30 \u2014 far worse than Node2Vec\n2. Hybrid methods all HURT performance (best is \u03b1=0.1 at -0.05 pp)\n3. PPI neighborhood Jaccard similarity is too coarse \u2014 2-hop neighborhoods average 4,828 genes\n4. Node2Vec already captures functional similarity that subsumes PPI proximity\n\nROOT CAUSE:\n- 2-hop PPI neighborhoods are very large (mean 4,828 genes) \u2192 high false positive overlap\n- PPI is too generic \u2014 same genes appear in many disease neighborhoods\n- DRKG already includes Gene-Disease and Drug-Gene edges \u2192 Node2Vec captures this\n\nCONCLUSION: External PPI data does NOT provide complementary signal to DRKG embeddings.",
      "result_metric": "PPI kNN 16.18% vs Node2Vec 36.93% \u2014 no improvement"
    },
    {
      "id": "h18",
      "title": "Withdrawn Drug Filter Enhancement",
      "category": "data",
      "rationale": "Validation found predictions for withdrawn drugs (Pergolide, Aducanumab). Comprehensive withdrawn drug list could improve practical value of predictions.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 13,
      "status": "validated",
      "steps": [
        "Step 1: Download FDA withdrawn drug list from openFDA",
        "Step 2: Cross-reference with DrugBank withdrawal status",
        "Step 3: Add to confidence_filter.py exclusion list",
        "Step 4: Re-run filtering on all predictions",
        "Step 5: Success criteria: 100% exclusion of withdrawn drugs"
      ],
      "findings": "h18 VALIDATED - Withdrawn drug filter already implemented in confidence_filter.py.\n\nCURRENT COVERAGE (18 patterns):\n1. WITHDRAWN_DRUG_PATTERNS (10): Safety withdrawals\n   - pergolide, cisapride, rofecoxib, valdecoxib, sibutramine\n   - propoxyphene, tegaserod, troglitazone, cerivastatin, phenylpropanolamine\n\n2. DISCONTINUED_DRUG_PATTERNS (7): Development stopped\n   - aducanumab, lexatumumab, fontolizumab, volociximab\n   - bectumomab, enokizumab, matuzumab\n\n3. REVOKED_APPROVAL_PATTERNS (1): FDA revocation\n   - olaratumab\n\nFUNCTIONALITY VERIFIED:\n- Pergolide \u2192 EXCLUDED (withdrawn)\n- Aducanumab \u2192 EXCLUDED (discontinued)\n- Olaratumab \u2192 EXCLUDED (revoked)\n- Aspirin \u2192 OK (normal drug)\n\nSUCCESS CRITERION MET: 100% exclusion of known withdrawn drugs.\n",
      "result_metric": "100% exclusion (18 patterns implemented)"
    },
    {
      "id": "h19",
      "title": "Disease Phenotype Similarity",
      "category": "feature",
      "rationale": "HIGH VALUE: External phenotype data could provide 23 pp improvement room. Drugs that work for similar diseases may work for the target disease. HPO (Human Phenotype Ontology) similarity could capture this.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 1,
      "status": "invalidated",
      "steps": [
        "Step 1: Download HPO annotations for diseases",
        "Step 2: Compute disease-disease similarity using HPO semantic similarity",
        "Step 3: For each drug-disease pair, find max similarity to known indications",
        "Step 4: Add as feature (careful about leakage - use training diseases only)",
        "Step 5: Evaluate R@30 on held-out diseases",
        "Step 6: Success criteria: >42.5% R@30"
      ],
      "findings": "INVALIDATED after comprehensive re-evaluation with 5-seed testing:\n\nCOVERAGE:\n- 799 DRKG diseases have HPO phenotype data (via MONDO\u2192OMIM/ORPHA mapping)\n- Only 119 (25.6%) of GT diseases have HPO + embeddings + GT\n- HPO-mapped diseases are 1.53x easier to predict (62.3% vs 40.7% for non-HPO)\n\nRESULTS (5-seed mean \u00b1 std):\n| Method                  | R@30          | Delta vs baseline |\n|-------------------------|---------------|-------------------|\n| Node2Vec kNN (baseline) | 36.91% \u00b1 5.59%| ---               |\n| HPO kNN (all diseases)  | 14.20% \u00b1 5.20%| -22.71 pp         |\n| HPO kNN (HPO subset)    | 34.84% \u00b115.99%| -2.07 pp          |\n| Hybrid \u03b1=0.5 (best)     | 37.19% \u00b1 5.63%| +0.28 pp          |\n\nKEY FINDINGS:\n1. HPO-only kNN achieves only 14.20% R@30 \u2014 far worse than Node2Vec\n2. On HPO-covered diseases specifically, Node2Vec (62.3%) > HPO (53.6%)\n3. Hybrid provides marginal +0.28 pp \u2014 within noise, not significant\n4. Correlation between HPO and Node2Vec similarity is low (0.126) \u2014 different signals, but HPO's is weaker\n5. Node2Vec already captures disease similarity well; HPO provides no complementary value\n\nROOT CAUSES:\n- HPO is focused on rare Mendelian diseases (OMIM/Orphanet sources)\n- HPO phenotype annotations are sparse for common diseases\n- Node2Vec random walks through DRKG already capture functional similarity\n- HPO Jaccard similarity is very sparse (mean 0.036)\n\nCONCLUSION: External phenotype data from HPO does NOT improve drug repurposing predictions.\nThe 23 pp gap to the oracle ceiling will NOT be closed by phenotype similarity.",
      "result_metric": "HPO kNN 14.20% vs Node2Vec 36.91% \u2014 no improvement; hybrid +0.28 pp (within noise)"
    },
    {
      "id": "h20",
      "title": "Ensemble Score Calibration",
      "category": "ensemble",
      "rationale": "Current boosting formula (Quad Boost) was derived heuristically. Proper calibration via Platt scaling or isotonic regression could improve score interpretation.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 20,
      "status": "blocked",
      "steps": [
        "Step 1: Hold out 20% of training pairs for calibration",
        "Step 2: Apply Platt scaling to raw GB scores",
        "Step 3: Evaluate calibration (Brier score, reliability diagram)",
        "Step 4: Compare calibrated vs uncalibrated R@30",
        "Step 5: Success criteria: Brier score < 0.15, maintain R@30"
      ],
      "findings": "BLOCKED (DRKG ceiling): Calibration improves scores but won't improve recall ceiling. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h21",
      "title": "Multi-Indication Drug Bonus",
      "category": "feature",
      "rationale": "Drugs with many indications may have pleiotropic effects. Adding indication_count as feature could help identify repurposing candidates.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 21,
      "status": "blocked",
      "steps": [
        "Step 1: Count known indications per drug from GT",
        "Step 2: Add indication_count as feature",
        "Step 3: Evaluate correlation with successful predictions",
        "Step 4: If positive, include in boosting formula",
        "Step 5: Success criteria: Positive correlation + >41.8% R@30"
      ],
      "findings": "BLOCKED (DRKG ceiling): DRKG-internal feature, won't help. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h22",
      "title": "Rare Disease Focus Evaluation",
      "category": "evaluation",
      "rationale": "Every Cure prioritizes rare diseases. Separate evaluation on rare vs common diseases could identify where model performs best and where to focus improvements.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 15,
      "status": "pending",
      "steps": [
        "Step 1: Classify diseases as rare (prevalence < 1/2000) using Orphanet",
        "Step 2: Evaluate R@30 separately for rare vs common diseases",
        "Step 3: Identify performance gaps",
        "Step 4: Document findings for prioritization",
        "Step 5: Success criteria: Establish rare disease baseline metrics"
      ],
      "findings": null,
      "result_metric": null
    },
    {
      "id": "h23",
      "title": "TxGNN Full Ranking Storage (GPU Required)",
      "category": "data",
      "rationale": "To enable TxGNN ensembles, store full drug rankings (not just top-50) for all diseases. This requires GPU to generate but enables offline ensemble evaluation.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 23,
      "status": "blocked",
      "steps": [
        "Step 1: Provision GPU on Vast.ai",
        "Step 2: Load TxGNN model (txgnn_500epochs.pt)",
        "Step 3: For each disease in ground truth, rank ALL 7954 drugs",
        "Step 4: Save as txgnn_full_rankings.csv (disease, drug, rank)",
        "Step 5: Re-test h1 and h2 with full rankings"
      ],
      "findings": "BLOCKED: Requires GPU resources",
      "result_metric": null
    },
    {
      "id": "h24",
      "title": "GB Model Error Analysis by Drug Class",
      "category": "evaluation",
      "rationale": "GB model achieves 42% overall but varies by drug class. Understanding which drug classes perform best/worst can guide targeted improvements without TxGNN.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 24,
      "status": "invalidated",
      "steps": [
        "Step 1: Classify all drugs by ATC code (1st level)",
        "Step 2: Calculate R@30 per drug class",
        "Step 3: Identify best performers (e.g., ACE inhibitors 66.7%)",
        "Step 4: Identify worst performers (e.g., biologics 27.3%)",
        "Step 5: Document patterns for targeted feature engineering"
      ],
      "findings": "SUPERSEDED: kNN is now best method, GB model analysis no longer relevant",
      "result_metric": null
    },
    {
      "id": "h25",
      "title": "Embedding Distance Calibration",
      "category": "feature",
      "rationale": "Current GB model uses raw embedding distances. Calibrating distances by drug/disease class could improve predictions for underperforming categories.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 25,
      "status": "blocked",
      "steps": [
        "Step 1: Compute embedding distances for all GT pairs",
        "Step 2: Compute distances for random negatives",
        "Step 3: Fit class-specific distance thresholds",
        "Step 4: Calibrate model predictions using class-specific priors",
        "Step 5: Evaluate R@30 improvement"
      ],
      "findings": "BLOCKED (DRKG ceiling): DRKG-internal calibration, won't help. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h26",
      "title": "Antibiotic Prediction Filtering Analysis",
      "category": "evaluation",
      "rationale": "The 13.6% figure was about antibiotics ranking poorly for infectious diseases. But evaluation shows model actually achieves 52% R@30 on infectious diseases. Need to understand: (1) which antibiotics are causing spurious predictions, (2) which non-infectious diseases they're predicted for, (3) whether existing confidence_filter.py adequately addresses this.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 16,
      "status": "pending",
      "steps": [
        "Step 1: Identify all antibiotic predictions in top-100 for non-infectious diseases",
        "Step 2: Categorize by antibiotic class and disease type",
        "Step 3: Analyze overlap with existing confidence_filter.py rules",
        "Step 4: Propose additional filtering rules if needed",
        "Step 5: Document patterns for future reference"
      ],
      "findings": null,
      "result_metric": null
    },
    {
      "id": "h27",
      "title": "Per-Category Baseline Documentation",
      "category": "evaluation",
      "rationale": "The discrepancy between reported 13.6% and actual 52% R@30 for infectious diseases suggests other category metrics may also be inaccurate. Need comprehensive baseline by disease category.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 17,
      "status": "pending",
      "steps": [
        "Step 1: Evaluate GB model on all disease categories (cancer, autoimmune, cardiovascular, etc.)",
        "Step 2: Record R@30 for each category with proper EC-to-DRKG mapping",
        "Step 3: Compare with figures in CLAUDE.md",
        "Step 4: Update CLAUDE.md with accurate category baselines",
        "Step 5: Identify categories that actually underperform for targeted improvement"
      ],
      "findings": null,
      "result_metric": null
    },
    {
      "id": "h28",
      "title": "DrugBank XML Indication Extraction",
      "category": "data",
      "rationale": "HIGH VALUE: More GT data expands kNN coverage. h4 showed 83% of identified missing FDA pairs hit@30, indicating value in GT expansion. Full DrugBank XML contains ~12K drug-indication pairs. Systematic extraction could add hundreds of missing pairs.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 1,
      "status": "blocked",
      "steps": [
        "Step 1: Download full DrugBank XML (requires academic license)",
        "Step 2: Parse indication data for all drugs",
        "Step 3: Map indications to MESH IDs using disease_name_matcher.py",
        "Step 4: Identify pairs not in current GT",
        "Step 5: Evaluate hit rate for new pairs",
        "Step 6: Add validated pairs to GT",
        "Success criteria: Identify >100 missing pairs with >70% hit@30"
      ],
      "findings": "BLOCKED: DrugBank XML requires academic license which we don't have access to. The vocabulary CSV only contains drug names/identifiers, not indication data. Would need to either: (1) Obtain DrugBank academic license, or (2) Find alternative indication data source.",
      "result_metric": null
    },
    {
      "id": "h29",
      "title": "Verify Node2Vec Held-Out Disease Generalization",
      "category": "evaluation",
      "rationale": "h5 revealed GB+TransE can't generalize to unseen diseases (45.89% -> 3-12% R@30 with disease holdout). CLAUDE.md claims Node2Vec+XGBoost achieved 41.9% on held-out diseases, but code review shows train_with_node2vec.py uses PAIR-LEVEL split, not disease-level. Must verify: does Node2Vec genuinely generalize, or was this also inflated?",
      "expected_impact": "high",
      "effort": "low",
      "priority": 1,
      "status": "validated",
      "steps": [
        "Step 1: Load Node2Vec embeddings (256-dim from DRKG)",
        "Step 2: Apply SAME disease-level 80/20 split used in h5",
        "Step 3: Train XGBoost on training diseases only",
        "Step 4: Evaluate R@30 on held-out test diseases",
        "Step 5: Compare with TransE-based model under identical conditions",
        "Step 6: If Node2Vec generalizes, determine WHY (embedding properties, dimensionality, training method)",
        "Success criteria: Establish honest baseline for novel disease generalization"
      ],
      "findings": "VALIDATED: Node2Vec+XGBoost DOES generalize to unseen diseases, significantly outperforming TransE.\n\nRESULTS (88 held-out test diseases, seed=42):\n\n| Experiment | R@30 | Notes |\n|---|---|---|\n| Existing GB+TransE (pair-trained) | 45.89% | Trained on ALL diseases (inflated) |\n| Existing Node2Vec (pair-trained) | 21.64% | Trained on ALL diseases |\n| Node2Vec+XGBoost concat (disease holdout) | 26.18% | RETRAINED, unseen diseases |\n| Node2Vec+XGBoost cpd (disease holdout) | 29.45% | RETRAINED, unseen diseases, BEST |\n| TransE+XGBoost cpd (disease holdout) | 15.90% | RETRAINED, unseen diseases |\n| Node2Vec Cosine (no ML) | 1.27% | No model needed |\n| TransE Cosine (no ML) | 0.00% | No model needed |\n\nKEY FINDINGS:\n1. Node2Vec 29.45% vs TransE 15.90% on disease-level holdout \u2014 Node2Vec is 1.85x better\n2. The '41.9% on held-out diseases' claim was INCORRECT \u2014 it used pair-level split\n3. Concat+product+diff features HELP Node2Vec: 26.18% -> 29.45% (+3.3 pp)\n4. Cosine similarity alone is near-useless (0-1.27%) \u2014 ML model IS needed\n5. All 4 positive controls pass for Node2Vec concat model (Metformin rank 22, Rituximab rank 21, Imatinib rank 12, Lisinopril rank 27)\n6. Node2Vec's 29.45% generalization is a REAL signal \u2014 much better than TransE's 3-12% from h5\n\nIMPLICATIONS:\n1. Node2Vec embeddings capture transferable drug-disease patterns that TransE does not\n2. The embedding method is the critical factor for generalization\n3. 29.45% is the honest generalization baseline for novel disease prediction\n4. There is substantial room for improvement through hybrid features or better embeddings\n5. Node2Vec's random walk method captures neighborhood structure better than TransE's translational model",
      "result_metric": "29.45% R@30 on held-out diseases (Node2Vec+XGBoost cpd) vs 15.90% (TransE+XGBoost cpd)"
    },
    {
      "id": "h30",
      "title": "Graph Feature-Based Generalization",
      "category": "architecture",
      "rationale": "h5 showed embedding-only features (concat/product/diff) don't generalize to unseen diseases. Graph structural features (degree, path count, shared neighbors) may generalize better because they capture TOPOLOGICAL patterns that transfer across diseases. A drug that shares gene targets with known treatments for SIMILAR diseases should generalize.",
      "expected_impact": "high",
      "effort": "high",
      "priority": 2,
      "status": "invalidated",
      "steps": [
        "Step 1: Load DRKG graph (data/drkg/drkg.tsv)",
        "Step 2: For each drug-disease pair, compute: shortest path, number of paths <=3 hops, shared gene neighbors, drug degree, disease degree",
        "Step 3: Train XGBoost on graph features ONLY (no embeddings) with disease-level holdout",
        "Step 4: Train XGBoost on graph features + embeddings with disease-level holdout",
        "Step 5: Evaluate R@30 on held-out diseases",
        "Step 6: Compare generalization of graph-only vs embedding-only vs combined",
        "Success criteria: >20% R@30 on held-out diseases (vs 3-12% for embedding-only)"
      ],
      "findings": "SUPERSEDED by h34: Graph topological features (degree, shared gene neighbors, Adamic-Adar, shortest path) were tested in h34 and provide NO improvement over Node2Vec once treatment-edge leakage is removed. Clean result: -0.18 pp. Graph features from DRKG are redundant with Node2Vec embeddings.",
      "result_metric": "See h34: 26.55% hybrid vs 26.73% Node2Vec only \u2014 no improvement"
    },
    {
      "id": "h31",
      "title": "Inductive Disease Representation via Gene-Disease Features",
      "category": "architecture",
      "rationale": "The GB model fails on unseen diseases because disease EMBEDDINGS are opaque vectors that the model memorizes. If instead we represent diseases by their GENE associations (from disease_genes.json), the model could learn transferable patterns: 'drugs targeting protein X help diseases involving gene Y'. This is inductive \u2014 new diseases with known gene associations can be scored.",
      "expected_impact": "high",
      "effort": "medium",
      "priority": 3,
      "status": "invalidated",
      "steps": [
        "Step 1: Load disease_genes.json and drug_targets.json",
        "Step 2: Create disease feature vector: binary/count of associated genes (or top-N PCA components)",
        "Step 3: Create drug feature vector: binary/count of target genes",
        "Step 4: Features for pair: target-gene overlap, shared pathway count, drug target PCA, disease gene PCA",
        "Step 5: Train XGBoost with disease-level holdout",
        "Step 6: Evaluate R@30 on held-out diseases",
        "Success criteria: >25% R@30 on held-out diseases"
      ],
      "findings": "SUPERSEDED: h35 tested gene features (shared genes, Jaccard, overlap coefficient) and found +0.73 pp at best. h41 tested gene overlap as disease similarity for kNN and it HURT (23.2% vs 36.8%). Gene-based representations from DRKG data provide no improvement over Node2Vec. Not worth implementing a more complex version (PCA of gene profiles) when the basic version already failed.",
      "result_metric": "Superseded by h35 (+0.73 pp) and h41 (gene similarity hurts kNN)"
    },
    {
      "id": "h32",
      "title": "Embedding Similarity Ranking (No ML Model)",
      "category": "evaluation",
      "rationale": "Perhaps the simplest approach is the right one: rank drugs by COSINE SIMILARITY to disease in embedding space, without training any ML model. This is inherently inductive (no disease-specific training needed). Previous attempts with TransE cosine caused data leakage, but with Node2Vec or properly evaluated TransE, simple similarity might be competitive.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 4,
      "status": "invalidated",
      "steps": [
        "Step 1: For each test disease, compute cosine similarity to ALL drugs using TransE embeddings",
        "Step 2: Rank drugs by similarity and compute R@30",
        "Step 3: Repeat with Node2Vec embeddings",
        "Step 4: Compare with GB model baseline",
        "Step 5: This requires NO training, so disease-level holdout is automatic",
        "Success criteria: Establish model-free baseline R@30"
      ],
      "findings": "INVALIDATED (tested as part of h29): Cosine similarity without ML is near-useless. Node2Vec cosine: 1.27% R@30, TransE cosine: 0.00% R@30. ML model IS required to learn drug-disease relationships from embeddings.",
      "result_metric": "Node2Vec cosine: 1.27% R@30, TransE cosine: 0.00% R@30"
    },
    {
      "id": "h33",
      "title": "Quantify Existing Model's True Generalization Gap",
      "category": "evaluation",
      "rationale": "h5 and h29 used a single disease split (seed=42). With h29 establishing 29.45% as baseline, multi-seed validation would strengthen confidence but is lower priority.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 20,
      "status": "invalidated",
      "steps": [
        "Step 1: Run Node2Vec+XGBoost evaluation with 5 different random seeds for disease splits",
        "Step 2: Run TransE+XGBoost with same 5 seeds",
        "Step 3: Compute mean and std R@30 for both",
        "Step 4: Verify 28.73% vs 16.64% gap is consistent",
        "Success criteria: Quantify gap with confidence intervals"
      ],
      "findings": "SUPERSEDED: Superseded by h40 multi-seed evaluation",
      "result_metric": null
    },
    {
      "id": "h34",
      "title": "Node2Vec + Graph Topological Features Hybrid",
      "category": "architecture",
      "rationale": "h29 established Node2Vec+XGBoost at 28.73% R@30 on disease-level holdout. Graph topological features (shortest path, shared neighbors, path counts) are inherently inductive and could provide COMPLEMENTARY signal to Node2Vec embeddings.",
      "expected_impact": "high",
      "effort": "high",
      "priority": 1,
      "status": "invalidated",
      "steps": [
        "Step 1: Load DRKG graph from data/drkg/drkg.tsv",
        "Step 2: For each drug-disease pair, compute: shortest path length, number of paths <=3 hops, shared gene neighbors, drug degree, disease degree",
        "Step 3: Combine graph features WITH Node2Vec embedding features (concat)",
        "Step 4: Train XGBoost on graph+Node2Vec features with disease-level holdout (seed=42)",
        "Step 5: Also test graph features ONLY (without Node2Vec) for comparison",
        "Step 6: Evaluate R@30 on same 88 held-out test diseases from h29",
        "Success criteria: >33% R@30 (>3.5 pp improvement over Node2Vec-only)"
      ],
      "findings": "INVALIDATED: Graph topological features do NOT improve generalization once treatment-edge leakage is removed.\n\nINITIAL RESULT (WITH LEAKAGE):\n- Node2Vec only: 30.0%\n- Graph only (including treatment edges): 37.77%\n- Node2Vec + Graph hybrid: 45.82% (+15.8 pp!)\n\nBut the 'direct_connection' feature (importance 0.044) encoded treatment edges from DRKG \u2014 which IS the label. DRKG contains 4,968 DRUGBANK::treats and 54,020 GNBR::T treatment edges that overlap with GT.\n\nCLEAN RESULT (treatment edges EXCLUDED from graph):\n- Node2Vec only: 26.73%\n- Graph only (clean): 14.39%\n- Node2Vec + Clean Graph: 26.55% (-0.18 pp \u2014 NO improvement)\n\nFeature importance (clean): Node2Vec 89.4%, Graph 10.6%. Drug_degree (0.072) dominates, but it doesn't help generalization.\n\nKEY INSIGHT: The 45.82% 'breakthrough' was entirely data leakage. Without treatment edges, graph features (degree, shared gene neighbors, Adamic-Adar) are uninformative for drug repurposing prediction. The graph features capture proximity which is already encoded in Node2Vec embeddings.",
      "result_metric": "26.55% R@30 (clean hybrid) vs 26.73% (Node2Vec only) \u2014 NO improvement after leakage removal"
    },
    {
      "id": "h35",
      "title": "Node2Vec + Gene-Disease Feature Hybrid",
      "category": "architecture",
      "rationale": "h29 showed Node2Vec generalizes at 28.73%. Gene-based features (drug target genes, disease-associated genes, target-gene overlap) are fully inductive. Combining with Node2Vec could improve generalization.",
      "expected_impact": "high",
      "effort": "medium",
      "priority": 2,
      "status": "invalidated",
      "steps": [
        "Step 1: Load drug_targets.json and disease_genes.json from data/reference/",
        "Step 2: For each drug-disease pair, compute: # shared target-disease genes, Jaccard similarity of gene sets, PCA of drug target profile, PCA of disease gene profile",
        "Step 3: Combine gene features WITH Node2Vec embedding features",
        "Step 4: Train XGBoost with disease-level holdout (seed=42)",
        "Step 5: Also test gene features ONLY for comparison",
        "Step 6: Evaluate R@30 on held-out diseases",
        "Success criteria: >33% R@30"
      ],
      "findings": "INVALIDATED: Gene features provide minimal additional value on top of Node2Vec embeddings.\n\nRESULTS (88 held-out test diseases, seed=42):\n- Node2Vec only (baseline): 25.82% R@30 (142/550 hits)\n- Gene features only: 7.91% R@30 (44/556 hits)\n- Node2Vec + Gene hybrid: 26.55% R@30 (146/550 hits)\n- Delta: +0.73 pp (marginal, NOT meeting >33% target)\n\nGene features tested (12 features): n_shared, jaccard, dice, overlap_coeff, n_drug_targets, n_disease_genes, has_drug_targets, has_disease_genes, has_both, log_drug/disease/shared.\n\nFEATURE IMPORTANCE (hybrid model):\n- Node2Vec: 87.7%\n- Genes: 12.3% (dominated by n_drug_targets at 0.096)\n- Most gene interaction features (shared, jaccard, dice) have near-zero importance\n\nDATA COVERAGE: 63.5% of GT diseases have gene data, 36.9% of drugs have target data. 6,353 shared genes between drug targets and disease genes.\n\nWHY IT FAILED:\n1. Gene features are too sparse \u2014 most drug-disease pairs have zero shared genes\n2. Drug target count is the only useful gene feature, but it's drug-level not pair-level\n3. Node2Vec already captures gene-mediated relationships implicitly through the knowledge graph structure\n4. The 12 gene features add noise relative to 512 Node2Vec features\n\nIMPLICATION: Simple gene overlap features don't add to Node2Vec. More sophisticated gene representations (pathway-level, PPI network distance, gene expression similarity) might help, but require additional data.",
      "result_metric": "26.55% R@30 (hybrid) vs 25.82% (Node2Vec only) \u2014 +0.73 pp, below 33% target"
    },
    {
      "id": "h36",
      "title": "Node2Vec Hyperparameter Tuning for Generalization",
      "category": "architecture",
      "rationale": "Current Node2Vec uses default parameters (dim=256). The p and q parameters control walk behavior: p<1 favors BFS-like local exploration, q<1 favors DFS-like outward exploration. Different p/q settings might produce embeddings that generalize better to unseen diseases. Also, walk length and number of walks affect embedding quality.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 6,
      "status": "blocked",
      "steps": [
        "Step 1: Identify current Node2Vec parameters (check if recorded in training logs)",
        "Step 2: Retrain Node2Vec with grid: p={0.5,1,2}, q={0.5,1,2} (9 combinations)",
        "Step 3: For each, train XGBoost and evaluate R@30 on disease-level holdout",
        "Step 4: Identify best p/q combination",
        "Step 5: Requires DRKG graph re-training \u2014 significant compute",
        "Success criteria: >33% R@30 with optimized parameters"
      ],
      "findings": "BLOCKED (DRKG ceiling): Node2Vec hyperparameter tuning won't break ceiling - h43 showed params already optimal. The 37% kNN ceiling (h39/h44) proves DRKG-internal improvements are exhausted.",
      "result_metric": null
    },
    {
      "id": "h37",
      "title": "Node2Vec Generalization Analysis by Disease Category",
      "category": "evaluation",
      "rationale": "h29 showed 28.73% overall R@30, but performance likely varies by disease category. Understanding which categories Node2Vec generalizes well/poorly for guides targeted improvement.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 3,
      "status": "validated",
      "steps": [
        "Step 1: Load h29 per-disease results from data/analysis/h29_node2vec_generalization_results.json",
        "Step 2: Categorize test diseases (cancer, autoimmune, cardiovascular, infectious, metabolic, neurological, etc.)",
        "Step 3: Compute per-category R@30 for Node2Vec vs TransE",
        "Step 4: Identify categories where Node2Vec excels vs fails",
        "Step 5: Compare with known category performance from CLAUDE.md (e.g., autoimmune 63%)",
        "Success criteria: Identify top 3 categories for improvement"
      ],
      "findings": "VALIDATED: Per-category analysis reveals extreme variation in Node2Vec generalization. TOP categories: ophthalmological 100% (2 diseases), hematological 70% (4), autoimmune 43.5% (6). MODERATE: cancer 19.3% (12), respiratory 31% (2). ZERO generalization: GI 0% (4), infectious 0% (5), rare/genetic 0% (2). Cardiovascular 10.2% (3) is a key gap. 49 diseases in 'other' category (19.1%) need better categorization. Best individuals: sarcoidosis 100%, autoimmune hemolytic anemia 100%, optic neuritis 100%. Worst: allergic rhinitis 0% (14 GT drugs), sepsis 0% (7 GT drugs). IMPLICATIONS: (1) Cardiovascular and infectious are highest-impact improvement targets, (2) Graph features (h34) may help sparse-connectivity diseases, (3) Gene features (h35) may help rare/genetic diseases.",
      "result_metric": "Overall 28.73% R@30; Best categories: ophthalmological 100%, hematological 70%, autoimmune 43.5%; Worst: GI/infectious/rare 0%"
    },
    {
      "id": "h38",
      "title": "XGBoost Hyperparameter Tuning for Generalization",
      "category": "architecture",
      "rationale": "All experiments used identical XGBoost params (100 estimators, max_depth=6, lr=0.1). These may overfit to training diseases. Shallower trees (max_depth=3-4), more trees (200-500), lower learning rate, and stronger regularization could improve generalization. This is lowest-effort improvement to try before architectural changes.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 1,
      "status": "validated",
      "steps": [
        "Step 1: Use Node2Vec+XGBoost (concat) as base",
        "Step 2: Grid search: max_depth={3,4,6,8}, n_estimators={100,200,500}, lr={0.01,0.05,0.1}, min_child_weight={1,5,10}, reg_alpha={0,0.1,1}, reg_lambda={1,5,10}",
        "Step 3: Evaluate each on disease-level holdout (same 88 test diseases)",
        "Step 4: Report best configuration and delta vs default",
        "Success criteria: >33% R@30"
      ],
      "findings": "VALIDATED (partial): Hyperparameter tuning improves by +3.09 pp but doesn't reach 33% target.\n\nStage 1 (depth/trees/lr, 36 configs): Best md=6 ne=500 lr=0.1: 30.91% R@30\nStage 2 (regularization, 27 configs): Best mcw=1 ra=1.0 rl=1: 31.09% R@30\nDefault (md=6 ne=100 lr=0.1): 28.00% R@30\n\nKEY FINDING: More trees (500 vs 100) is the biggest driver (+2.91 pp). L1 regularization (alpha=1.0) adds +0.18 pp. Shallower trees (md=3,4) hurt. Deeper trees (md=8) don't help beyond md=6.\n\nBest config: max_depth=6, n_estimators=500, lr=0.1, min_child_weight=1, reg_alpha=1.0, reg_lambda=1",
      "result_metric": "31.09% R@30 (best tuned) vs 28.00% (default) \u2014 +3.09 pp"
    },
    {
      "id": "h39",
      "title": "Disease Similarity Transfer Learning",
      "category": "architecture",
      "rationale": "h34/h35 showed that augmenting Node2Vec with explicit features from the SAME knowledge graph provides no improvement. But we haven't tried TRANSFERRING knowledge from similar diseases. For a new test disease, we could weight training examples from the most similar training diseases (by Node2Vec cosine similarity). This is a form of meta-learning within the existing framework.",
      "expected_impact": "high",
      "effort": "medium",
      "priority": 2,
      "status": "validated",
      "steps": [
        "Step 1: For each test disease, compute Node2Vec cosine similarity to all training diseases",
        "Step 2: Weight training examples by similarity (closer diseases get higher weight)",
        "Step 3: Train disease-specific or similarity-weighted XGBoost models",
        "Step 4: Compare with uniform-weight baseline",
        "Step 5: Alternatively, use k-nearest training diseases only (k=5,10,20)",
        "Success criteria: >33% R@30"
      ],
      "findings": "VALIDATED: Disease similarity transfer provides MAJOR improvement. Best method: kNN collaborative filtering (Approach B).\n\nMULTI-SEED RESULTS (5 seeds: 42, 123, 456, 789, 1024):\n\n| Method | Mean R@30 | Std | vs Baseline |\n|---|---|---|---|\n| XGBoost tuned (baseline) | 26.57% | \u00b13.98% | --- |\n| Approach A (global + sim features) | 30.55%* | --- | +3.98 pp |\n| Approach B (kNN k=20, no ML) | 37.04% | \u00b15.81% | +10.47 pp |\n| Approach C (local weighted models, k=10 t=2) | 31.22% | \u00b15.83% | +4.65 pp |\n| Approach D (XGBoost + kNN feature) | 34.08% | \u00b16.00% | +7.51 pp |\n| Rank fusion (alpha=0.1) | 37.16% | \u00b15.12% | +10.59 pp |\n\n*Approach A single-seed only (seed=42).\n\nAPPROACH B DETAILS:\n- For each test disease, find k=20 nearest training diseases by Node2Vec cosine similarity\n- Rank drugs by similarity-weighted frequency among those diseases\n- No ML model needed \u2014 purely collaborative filtering\n- Paired t-test vs baseline: t=7.11, p=0.002 (highly significant), Cohen's d=3.18\n\nDIAGNOSTICS:\n- Candidate pool: ~59 drugs per test disease (from 20 nearest training diseases)\n- GT coverage: 40.9% of GT drugs found in candidate pool on average\n- 44.3% of test diseases have 0% GT coverage (truly novel diseases)\n- kNN dominates hybrid: alpha=0.1 (90% kNN / 10% XGBoost) is optimal\n\nKEY INSIGHTS:\n1. Similar diseases share treatments \u2014 this is the dominant signal for drug repurposing\n2. XGBoost model adds negligible value on top of kNN collaborative filtering\n3. The method is limited: can only recommend drugs that appear in similar diseases' GT\n4. For truly novel diseases (no similar training neighbors), method falls back to zero\n5. The ~37% mean is driven by the 56% of diseases that HAVE similar training counterparts",
      "result_metric": "kNN k=20: 37.04% \u00b1 5.81% mean R@30 (5 seeds), +10.47 pp over baseline (p=0.002)"
    },
    {
      "id": "h40",
      "title": "Multi-Seed Disease Holdout Stability Check",
      "category": "evaluation",
      "rationale": "All results use seed=42. The 28.73% baseline might be unusually high or low for this split. Running 5 seeds would give mean +/- std. Important for establishing reliable baseline before trying more complex approaches.",
      "expected_impact": "low",
      "effort": "low",
      "priority": 3,
      "status": "validated",
      "steps": [
        "Step 1: Run Node2Vec+XGBoost (concat) with seeds 42,123,456,789,1024",
        "Step 2: Report mean, std, min, max R@30",
        "Step 3: If variance is high (>5 pp), results from single-seed experiments are unreliable",
        "Success criteria: Establish confidence interval for 28.73%"
      ],
      "findings": "VALIDATED: Multi-seed analysis reveals moderate variance and confirms tuning gain is real.\n\nDEFAULT XGBoost (md=6, ne=100, lr=0.1):\n- Seeds: 27.82%, 22.92%, 24.03%, 26.73%, 17.16%\n- Mean: 23.73% \u00b1 3.73%\n- Range: [17.16%, 27.82%]\n\nTUNED XGBoost (h38: md=6, ne=500, lr=0.1, ra=1.0):\n- Seeds: 30.00%, 26.43%, 26.27%, 28.34%, 18.21%\n- Mean: 25.85% \u00b1 4.06%\n- Range: [18.21%, 30.00%]\n\nKEY FINDINGS:\n1. Seed 42 was LUCKY \u2014 highest for default (27.82%), near-highest for tuned (30.00%). Previously reported 28.73%/31.09% were inflated.\n2. TRUE MEAN BASELINE: 23.73% (default) / 25.85% (tuned) \u2014 not 28.73%/31.09%.\n3. TUNING IS REAL: Tuned wins 5/5 seeds. Paired t-test: t=5.174, p=0.0066 (significant). Mean improvement: +2.12 pp.\n4. VARIANCE IS MODERATE: 3.7-4.1 pp std. Disease split matters significantly \u2014 seed 1024 gives ~10 pp lower than seed 42.\n5. All future experiments should use multi-seed evaluation to avoid misleading single-seed results.",
      "result_metric": "Default mean: 23.73% \u00b1 3.73%, Tuned mean: 25.85% \u00b1 4.06%, Tuning improvement: +2.12 pp (p=0.0066)"
    },
    {
      "id": "h41",
      "title": "Improved Disease Similarity Measure",
      "category": "architecture",
      "rationale": "h39 showed kNN collaborative filtering at 37% R@30 using Node2Vec cosine similarity for disease-disease comparison. But Node2Vec embeddings were not optimized for disease similarity \u2014 they capture general graph proximity. A disease similarity measure that accounts for shared gene associations, phenotype overlap, or treatment overlap (from training only) could improve kNN performance.",
      "expected_impact": "high",
      "effort": "medium",
      "priority": 1,
      "status": "invalidated",
      "steps": [
        "Step 1: Compute disease-disease similarity using: (a) Node2Vec cosine, (b) shared gene overlap (from disease_genes.json), (c) shared drug overlap (training GT only), (d) combined similarity",
        "Step 2: Run kNN k=20 approach B with each similarity measure",
        "Step 3: Multi-seed evaluation (5 seeds)",
        "Step 4: Test hybrid similarity (weighted combination of measures)",
        "Success criteria: >40% mean R@30 (>3 pp over kNN baseline)"
      ],
      "findings": "INVALIDATED: No fair (inductive) similarity measure beats Node2Vec for kNN.\n\nMULTI-SEED RESULTS (5 seeds):\n- Node2Vec cosine: 36.76% \u00b1 5.86% (BASELINE)\n- Gene overlap (Jaccard): 23.20% \u00b1 2.68% (-13.56 pp, MUCH WORSE)\n- Combined N2V + gene: 37.35% \u00b1 5.77% (+0.59 pp, negligible)\n- Drug overlap (TRANSDUCTIVE): 59.11% \u00b1 4.52% (+22.35 pp, UNFAIR)\n- Combined N2V + drug (TRANSDUCTIVE): 47.42% \u00b1 5.11%\n- Combined all (TRANSDUCTIVE): 49.28% \u00b1 5.56%\n\nKEY INSIGHTS:\n1. Gene overlap HURTS: worse alone and barely improves combined with Node2Vec\n2. Drug overlap = ORACLE: 59% R@30 shows collaborative filtering ceiling when you know a disease's drugs\n3. Node2Vec is the best FAIR similarity measure available\n4. The 37% kNN performance may be near the ceiling for inductive approaches using DRKG",
      "result_metric": "Node2Vec cosine = best fair measure (36.76% \u00b1 5.86%). Gene overlap = 23.20%. No improvement found."
    },
    {
      "id": "h42",
      "title": "kNN + XGBoost Rescue for Novel Diseases",
      "category": "architecture",
      "rationale": "h39 showed 44% of test diseases have zero GT coverage from kNN (no similar training diseases). For these 'orphan' diseases, kNN returns zero hits. A hybrid could use kNN for diseases with close neighbors (max_sim > threshold) and XGBoost for diseases without. This targets the 44% failure cases without hurting the 56% where kNN excels.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 2,
      "status": "invalidated",
      "steps": [
        "Step 1: For each test disease, compute max similarity to any training disease",
        "Step 2: If max_sim > threshold, use kNN approach. Else, use XGBoost",
        "Step 3: Test thresholds: 0.5, 0.6, 0.7, 0.8",
        "Step 4: Multi-seed evaluation",
        "Success criteria: >38% mean R@30 (>1 pp over pure kNN)"
      ],
      "findings": "INVALIDATED: XGBoost rescue provides no benefit. At threshold=0.3, ALL 88 test diseases have max_sim > 0.3, so 100% are routed to kNN (= pure kNN = 37.07%). As threshold increases, more diseases go to XGBoost, and performance DECREASES: threshold 0.5 = 34.15%, 0.7 = 31.60%, 0.8 = 29.43%. XGBoost is worse than kNN for ALL disease similarity ranges, not just dissimilar diseases. The hypothesis that XGBoost helps for 'orphan' diseases was wrong \u2014 kNN is superior even for diseases with low similarity to training neighbors.",
      "result_metric": "Best hybrid = pure kNN (37.07%). XGBoost rescue only hurts: threshold 0.5 = 34.15%, 0.7 = 31.60%"
    },
    {
      "id": "h43",
      "title": "kNN with Expanded k and Drug Count Normalization",
      "category": "architecture",
      "rationale": "h39 used k=20 but didn't systematically optimize k with multi-seed evaluation. Also, diseases with more GT drugs contribute more drug candidates, potentially biasing toward common diseases. Normalizing drug scores by disease drug count might improve.",
      "expected_impact": "medium",
      "effort": "low",
      "priority": 3,
      "status": "invalidated",
      "steps": [
        "Step 1: Test k = 10, 15, 20, 25, 30, 40, 50 with multi-seed evaluation",
        "Step 2: Test normalization: (a) raw count, (b) divided by disease drug count, (c) divided by sqrt(count)",
        "Step 3: Test similarity weighting: (a) linear, (b) exponential, (c) top-k equal weight",
        "Success criteria: >38% mean R@30"
      ],
      "findings": "INVALIDATED: The h39 default (k=20, raw scores, linear similarity weighting) is already optimal. Swept 72 configs (8 k-values x 3 normalizations x 3 weightings). Multi-seed on top 5: all equal or worse than default (37.00%). Normalization and smaller k both hurt. Exponential weighting = linear. No configuration beat baseline.",
      "result_metric": "Best = h39 default (37.00% \u00b1 5.75%). No improvement."
    },
    {
      "id": "h44",
      "title": "Transductive kNN with All Diseases",
      "category": "architecture",
      "rationale": "Current kNN uses disease-level holdout for evaluation honesty. But in production, we'd use ALL known diseases as the kNN pool. Testing this 'transductive' setting (where test diseases also contribute to the pool through their known drugs) would give us the upper bound of what's achievable with collaborative filtering on the full dataset.",
      "expected_impact": "high",
      "effort": "low",
      "priority": 4,
      "status": "validated",
      "steps": [
        "Step 1: Use full GT (no holdout) as kNN pool",
        "Step 2: For each disease, find k nearest OTHER diseases (exclude self)",
        "Step 3: Rank drugs, evaluate R@30 (leave-one-out style)",
        "Step 4: This is NOT generalizable but establishes collaborative filtering ceiling",
        "Success criteria: Establish upper bound for kNN approach"
      ],
      "findings": "VALIDATED: Transductive kNN establishes clear ceiling.\n\nNode2Vec kNN (leave-one-out, 440 diseases):\n- k=5: 28.49%, k=10: 34.09%, k=15: 34.83%, k=20: 35.91%\n- k=25: 36.58%, k=30: 37.07% (BEST), k=50: 35.00%, k=100: 31.08%\n\nOracle (drug overlap similarity):\n- k=10: 59.99%, k=20: 60.34%, k=50: 60.41%\n\nKEY FINDINGS:\n1. Transductive k=30 (37.07%) \u2248 Inductive k=20 (37.04%). More diseases in pool doesn't help \u2014 similarity quality is the bottleneck, not pool size.\n2. Optimal k=25-30 for transductive (same range as inductive).\n3. Oracle ceiling: 60.4% R@30 \u2014 the maximum achievable with perfect disease similarity.\n4. Gap: 37% (Node2Vec) vs 60% (Oracle) = 23 pp improvement possible through better similarity.\n5. Diminishing returns beyond k=30 \u2014 too many neighbors introduce noise.",
      "result_metric": "Transductive: 37.07% R@30 (k=30). Oracle: 60.4%. Gap: 23 pp improvement room."
    },
    {
      "id": "h45",
      "title": "Learned Disease Similarity Metric",
      "category": "architecture",
      "rationale": "h44 showed 23 pp gap between Node2Vec kNN (37%) and oracle (60%). Node2Vec cosine is a GENERIC similarity \u2014 not optimized for drug overlap prediction. Training a model to predict disease-disease drug overlap FROM Node2Vec features could produce a SPECIALIZED similarity metric that closes this gap. This is metric learning applied to kNN.",
      "expected_impact": "high",
      "effort": "medium",
      "priority": 1,
      "status": "invalidated",
      "steps": [
        "Step 1: For each pair of training diseases, compute drug overlap Jaccard as target variable",
        "Step 2: Use Node2Vec embedding features (concat, diff, product) as input",
        "Step 3: Train XGBoost regressor to predict drug overlap from embeddings",
        "Step 4: Use predicted drug overlap as similarity measure for kNN on test diseases",
        "Step 5: Multi-seed evaluation. Compare with Node2Vec cosine baseline.",
        "Success criteria: >40% mean R@30 (>3 pp over kNN baseline)"
      ],
      "findings": "INVALIDATED: Learned similarity is WORSE than cosine (-3.98 pp, p=0.008).\n\nMULTI-SEED RESULTS:\n- Learned: 33.21% \u00b1 5.55%\n- Cosine: 37.19% \u00b1 5.79%\n- Delta: -3.98 pp (learned is WORSE)\n- Paired t-test: t=-4.898, p=0.0081 (significantly worse)\n\nThe XGBoost regressor overfits to training disease pairs. It learns patterns specific to SEEN diseases that don't transfer to unseen ones \u2014 the same generalization failure seen with XGBoost classification (h5). Cosine similarity is more robust because it's parameter-free and doesn't overfit.\n\nIMPLICATION: Learned metrics from DRKG data can't beat cosine. The 23 pp gap to oracle requires EXTERNAL information (phenotype, literature, clinical) not derivable from the knowledge graph alone.",
      "result_metric": "Learned: 33.21% \u00b1 5.55% (WORSE than cosine 37.19% by -3.98 pp, p=0.008)"
    },
    {
      "id": "h46",
      "title": "Drug-Centric Repurposing (Flip the Problem)",
      "category": "architecture",
      "rationale": "kNN is disease-centric: find similar diseases, recommend their drugs. For drugs with many indications (polypharmacy signals), we could flip: find similar drugs, recommend their diseases. This may help diseases with sparse training neighbors.",
      "expected_impact": "medium",
      "effort": "medium",
      "priority": 2,
      "status": "invalidated",
      "steps": [
        "Step 1: Compute drug-drug similarity using Node2Vec embeddings",
        "Step 2: For each test disease-drug pair, find k nearest drugs to candidate drug",
        "Step 3: Score by overlap with training disease's treatments",
        "Step 4: Combine with disease-centric kNN scores",
        "Step 5: Evaluate R@30 on held-out diseases",
        "Step 6: Success criteria: >38% R@30"
      ],
      "findings": "h46 INVALIDATED - Conceptually flawed for disease-holdout evaluation.\n\nANALYSIS:\nOur evaluation setup is disease-holdout:\n- Given: test DISEASE (not seen in training)\n- Task: rank all DRUGS\n\nDrug-centric approach would help if problem was drug-holdout:\n- Given: test DRUG (not seen in training)  \n- Task: rank all DISEASES\n\nFor drug-centric to help disease-holdout, the reasoning would be:\n\"If drug C is similar to drug D, and D treats train disease T,\n and T is similar to test disease D_test, then C might treat D_test\"\n\nBut this is just two-hop graph similarity which is already captured by Node2Vec embeddings!\nNode2Vec encodes graph structure including drug-disease-drug paths.\n\nADDITIONAL DATA:\n- Drugs with \u22655 indications: only 135 (12% of drugs)\n- Drugs with \u226510 indications: only 34 (3% of drugs)\n- Most drugs have 1-2 indications, limiting drug-centric transfer\n\nCONCLUSION: Drug-centric framing doesn't add signal beyond what's already in Node2Vec embeddings.\n",
      "result_metric": "N/A - conceptually flawed"
    },
    {
      "id": "h47",
      "title": "Zero-Shot Disease Prediction via Drug Properties",
      "category": "architecture",
      "rationale": "44% of test diseases have 0% GT coverage in kNN pool. For these, similarity-based methods fail. Use drug properties (targets, pathways, ATC) to predict without disease similarity.",
      "expected_impact": "medium",
      "effort": "high",
      "priority": 8,
      "status": "invalidated",
      "steps": [
        "Step 1: Identify test diseases with 0% kNN coverage",
        "Step 2: Build drug property features (targets, pathways, ATC codes)",
        "Step 3: Train model to predict disease-drug compatibility from drug properties alone",
        "Step 4: Use for zero-shot diseases, kNN for others",
        "Step 5: Evaluate R@30 on the combined approach",
        "Step 6: Success criteria: >5% R@30 on zero-shot diseases"
      ],
      "findings": "h47 INVALIDATED - Limited impact ceiling makes zero-shot low ROI.\n\nANALYSIS RESULTS:\n- Zero-coverage diseases: ~42% of test diseases (consistent across 5 seeds)\n- BUT only ~15% of test GT pairs (most zero-coverage diseases have few GT drugs)\n- Impact ceiling: Even 10% zero-shot recall only adds +1.5 pp to overall R@30\n- kNN achieves 37% R@30 on 85% of GT pairs; zero-shot cannot meaningfully improve this\n\nKEY INSIGHT: Zero-coverage diseases are edge cases with small GT counts.\nThe 39 diseases with 0% coverage only contribute 63 GT pairs total (avg 1.6 drugs/disease).\nCompare to non-zero diseases: 49 diseases with 487 GT pairs (avg 9.9 drugs/disease).\n\nCATEGORY BREAKDOWN:\n1. K-nearest selection failures (29 diseases): GT drugs exist in training pool but k=20 neighbors don't include right diseases. These could potentially be addressed by better similarity measures.\n2. Truly novel drugs (10 diseases): 9 drugs not in any training disease GT. These are fundamentally hard.\n\nRECOMMENDATION: Deprioritize zero-shot approaches. Focus on improving kNN coverage for the majority of GT pairs.\n",
      "result_metric": "CEILING: +1.5 pp max impact from zero-shot"
    },
    {
      "id": "h48",
      "title": "kNN Coverage Analysis (Diagnosis)",
      "category": "evaluation",
      "rationale": "Before trying new architectures, understand exactly why kNN fails. What % of test GT drugs appear in similar diseases' treatments? How does coverage correlate with R@30?",
      "expected_impact": "low",
      "effort": "low",
      "priority": 4,
      "status": "validated",
      "steps": [
        "Step 1: For each test disease, compute overlap between GT drugs and kNN pool",
        "Step 2: Correlate coverage with per-disease R@30",
        "Step 3: Identify disease categories with poor coverage",
        "Step 4: Document findings to guide architecture choice",
        "Step 5: Success criteria: Establish coverage-performance relationship"
      ],
      "findings": "VALIDATED: kNN ceiling is caused by GT drug coverage sparsity.\n\nKEY FINDINGS:\n- 44.3% of test diseases have 0% GT drug coverage in kNN pool\n- Correlation (coverage vs recall) = 0.898 \u2014 coverage almost perfectly predicts success\n- Diseases with 81-100% coverage achieve 78.2% R@30 (vs 37% overall)\n- Diseases with 0% coverage achieve 0% R@30 (by definition)\n\nIMPLICATIONS:\n1. kNN is optimal for its coverage \u2014 the algorithm is not the bottleneck\n2. The 37% ceiling is fundamentally about GT sparsity, not method quality\n3. Improvement requires:\n   a) More GT data (DrugBank/ChEMBL indications) to increase coverage\n   b) Drug-centric approach for zero-coverage diseases (h46)\n   c) Zero-shot methods for diseases with no similar training diseases (h47)\n\nCOVERAGE DISTRIBUTION (n=88 test diseases):\n| Coverage | Count | % | Mean R@30 |\n|----------|-------|---|-----------|\n| 0%       | 39    | 44%| 0.0%     |\n| 1-60%    | 16    | 18%| 28.4%    |\n| 61-100%  | 33    | 38%| 74.2%    |\n\nThe ~38% of diseases with good coverage drive the 37% overall R@30.",
      "result_metric": "44.3% diseases with 0% coverage, correlation 0.898"
    }
  ],
  "completed": [
    "h1",
    "h3",
    "h5",
    "h29",
    "h32",
    "h34",
    "h35",
    "h37",
    "h38",
    "h40",
    "h30",
    "h39",
    "h42",
    "h43",
    "h41",
    "h44",
    "h45",
    "h31",
    "h6",
    "h7",
    "h11",
    "h12",
    "h15",
    "h20",
    "h21",
    "h25",
    "h36",
    "h33",
    "h24",
    "h23"
  ],
  "learnings": [
    {
      "date": "2026-01-25",
      "finding": "Fuzzy disease matching improved R@30 from 37.4% to 41.8%",
      "source": "CLAUDE.md"
    },
    {
      "date": "2026-01-25",
      "finding": "Quad Boost features (target, ATC, chemical, pathway) are CIRCULAR and inflate metrics",
      "source": "CLAUDE.md"
    },
    {
      "date": "2026-01-25",
      "finding": "TxGNN achieves 83.3% R@30 on storage diseases but only 6.7% overall",
      "source": "txgnn_learnings.md"
    },
    {
      "date": "2026-01-25",
      "finding": "Biologic gap: mAbs have 2.1 diseases/drug vs 11.1 for small molecules",
      "source": "CLAUDE.md"
    },
    {
      "date": "2026-01-25",
      "finding": "Infectious disease paradox: more training data correlates with WORSE performance",
      "source": "CLAUDE.md"
    },
    {
      "date": "2026-01-25",
      "finding": "Validation precision: 22.5% for top predictions (batches 1+2)",
      "source": "validation_sessions.md"
    },
    {
      "date": "2026-01-26",
      "hypothesis_id": "h1",
      "finding": "TxGNN pre-computed predictions contain only top-50 drugs per disease. GT drugs are NOT in top-50 for most diseases (0% coverage). Ensemble blocked without GPU inference.",
      "implication": "h2 (Category-Routed Ensemble) has same blocker. TxGNN-based ensembles require live GPU inference, not pre-computed files."
    },
    {
      "date": "2026-01-26",
      "hypothesis_id": "h3",
      "finding": "The 13.6% infectious disease recall was antibiotic CLASS performance, not disease-level evaluation. Actual general model R@30 on infectious diseases is 52.0%.",
      "implication": "Specialist model unnecessary - general model already performs well. The real problem is spurious antibiotic predictions for non-infectious diseases, already handled by confidence_filter.py."
    },
    {
      "date": "2026-01-26",
      "hypothesis_id": "h4",
      "finding": "All 4,968 DRKG treatment edges are already in the 58K-pair GT. Only 6 additional FDA pairs found via manual search, 5/6 hit@30. Impact: +0.22 pp.",
      "implication": "GT expansion from public databases provides diminishing returns. Every Cure GT is comprehensive. Future expansion requires proprietary indication databases or careful manual curation."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h5",
      "finding": "CRITICAL FINDING: GB model with TransE embedding features (concat/product/diff) cannot generalize to held-out diseases. ALL 5 negative sampling strategies collapsed from 45.89% to 3-12% R@30 on disease-level holdout. The existing model's 41.8% R@30 is within-distribution performance, not novel disease generalization.",
      "implication": "This fundamentally changes the research direction: (1) Feature engineering and negative sampling improvements are IRRELEVANT if the model can't generalize; (2) The reported 41.8% R@30 is inflated \u2014 true generalization performance is much lower; (3) The Node2Vec approach (reportedly 41.9% on held-out diseases) may use a fundamentally different/better evaluation or embedding approach; (4) Future work must focus on architecture changes (better embeddings, graph features, or inductive models) rather than training data improvements."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h29",
      "finding": "Node2Vec+XGBoost achieves 28.73% R@30 on disease-level holdout (88 test diseases), vs 16.64% for TransE+XGBoost. The '41.9% on held-out diseases' claim was incorrect \u2014 original code used pair-level split. Node2Vec's random walk embeddings capture transferable patterns that TransE's translational model does not. Concat vs full features (concat+product+diff) make no difference for Node2Vec (both 28.73%). Cosine similarity alone is useless (0-1.27%).",
      "implication": "Node2Vec is the better embedding for generalization (1.73x TransE). The 28.73% establishes the honest generalization baseline. Future work should: (1) Use Node2Vec as the default embedding, (2) Investigate why Node2Vec generalizes better (neighborhood structure vs translational), (3) Improve beyond 28.73% through graph features, gene-based features, or better embeddings (e.g., augmented Node2Vec with more walk parameters), (4) Consider hybrid Node2Vec + graph feature approaches."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h35",
      "finding": "Gene-based features (shared genes, Jaccard, Dice, overlap coefficient, target counts) provide minimal additional value on top of Node2Vec: +0.73 pp (25.82% -> 26.55%). Gene-only model: 7.91% R@30. Feature importance: Node2Vec 87.7%, genes 12.3%. Most gene interaction features have near-zero importance; only n_drug_targets matters (0.096).",
      "implication": "Simple gene overlap features don't improve generalization because: (1) most drug-disease pairs have zero shared genes (sparsity), (2) Node2Vec already captures gene-mediated relationships implicitly through graph structure. More sophisticated representations needed: pathway-level features, PPI network distances, or gene expression similarity. Focus effort on graph topological features (h34) rather than gene-level features."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h37",
      "finding": "Node2Vec generalization varies dramatically by disease category: ophthalmological 100%, hematological 70%, autoimmune 43.5%, cancer 19.3%, cardiovascular 10.2%, infectious 0%, GI 0%, rare/genetic 0%. Dense KG connectivity correlates with generalization success.",
      "implication": "Improvement efforts should target: (1) cardiovascular diseases (10.2% with 59 GT drugs \u2014 high volume, moderate performance), (2) infectious diseases (0% with 14 GT drugs \u2014 complete failure), (3) GI diseases (0% with 9 GT drugs). Dense diseases (autoimmune, hematological) already work well. Graph topological features (h34) may help sparse-connectivity disease categories."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h34",
      "finding": "Graph topological features (degree, shared gene neighbors, Adamic-Adar) do NOT improve over Node2Vec once treatment-edge leakage is removed. Initial result of 45.82% was entirely due to direct_connection feature encoding treatment edges from DRKG (4,968 DRUGBANK::treats + 54,020 GNBR::T edges). Clean result: 26.55% hybrid vs 26.73% Node2Vec-only (-0.18 pp).",
      "implication": "CRITICAL FINDING: Feature engineering from the SAME knowledge graph used for embeddings provides NO additional signal. Node2Vec already captures all relevant graph structure. Improvement must come from: (1) DIFFERENT data sources (clinical trials, literature, gene expression), (2) DIFFERENT embedding methods (GNN, attention-based), (3) DIFFERENT architectures (multi-task, meta-learning). The 28.73% Node2Vec baseline may be near the ceiling for DRKG-only approaches."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h40",
      "finding": "Multi-seed evaluation (5 seeds) reveals: Default mean 23.73% \u00b1 3.73%, Tuned mean 25.85% \u00b1 4.06%. Seed 42 was lucky (highest). Tuning wins all 5 seeds (p=0.0066). Previously reported 28.73%/31.09% were from the best seed.",
      "implication": "CRITICAL: (1) True generalization baseline is ~24-26%, not 29-31%. (2) XGBoost tuning is a REAL improvement (+2.12 pp, significant). (3) Future experiments MUST use multi-seed evaluation. (4) Single-seed experiments have ~\u00b14 pp noise, making improvements <4 pp unreliable without multi-seed validation."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h39",
      "finding": "kNN collaborative filtering (k=20 nearest training diseases, similarity-weighted drug frequency) achieves 37.04% \u00b1 5.81% mean R@30, a +10.47 pp improvement over XGBoost baseline (26.57%). Improvement is highly significant (p=0.002, d=3.18). XGBoost model adds negligible value on top of kNN. Hybrid rank fusion (alpha=0.1) matches pure kNN at 37.16%.",
      "implication": "PARADIGM SHIFT: (1) 'Similar diseases share treatments' is the dominant signal, stronger than learned embeddings. (2) Drug repurposing may be better framed as collaborative filtering than as an ML classification task. (3) Pure kNN outperforms all ML models tested so far. (4) Key limitation: kNN can only recommend drugs already in GT for similar diseases \u2014 cannot discover truly novel drug-disease pairs. (5) Future work should focus on: improving disease similarity measures, expanding the training disease pool, and developing methods for diseases with no close training neighbors."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h41",
      "finding": "Gene overlap similarity is WORSE than Node2Vec for kNN (23.20% vs 36.76%). Combined N2V+gene gives negligible improvement (+0.59 pp). Drug overlap (transductive/oracle) reaches 59.11% showing the ceiling. No fair inductive similarity measure beats Node2Vec cosine.",
      "implication": "Node2Vec cosine is the best available disease similarity for kNN. The 37% R@30 may be near the inductive ceiling for DRKG-based approaches. Breaking through requires: (1) External disease similarity data (phenotype ontology, literature co-occurrence), (2) Better disease embeddings trained specifically for similarity, (3) Expanding the GT disease pool so more diseases have close neighbors."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h42",
      "finding": "XGBoost rescue for diseases without close kNN neighbors provides NO benefit. ALL test diseases have max_sim > 0.3 to some training disease. kNN outperforms XGBoost even for diseases with low similarity (0.3-0.5). XGBoost adds no value in any setting.",
      "implication": "The XGBoost model is completely dominated by kNN collaborative filtering. This reinforces h39's finding that ML on embeddings is inferior to direct similarity-based drug transfer for this task."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h43",
      "finding": "Swept 72 kNN configurations (8 k-values \u00d7 3 normalizations \u00d7 3 weightings). Default config (k=20, raw, linear) is already optimal. All alternatives equal or worse.",
      "implication": "kNN parameters are already at their optimum. Further improvements must come from changing the similarity measure or the underlying data, not hyperparameter tuning."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h44",
      "finding": "Transductive kNN (leave-one-out, 440 diseases) achieves 37.07% at k=30, nearly identical to inductive (37.04%). Oracle ceiling (drug overlap similarity) is 60.4%. Gap: 23 pp between Node2Vec similarity and perfect similarity.",
      "implication": "Pool size is NOT the bottleneck \u2014 similarity quality is. The 23 pp gap to oracle shows enormous room for improvement IF we can find better disease similarity measures. Since gene overlap fails (h41), the path forward is: (1) learned similarity metrics, (2) external phenotype/literature data, or (3) fundamentally different disease representation."
    },
    {
      "date": "2026-01-27",
      "hypothesis_id": "h45",
      "finding": "Learned disease similarity (XGBoost regressor predicting drug overlap from Node2Vec features) is WORSE than cosine by -3.98 pp (33.21% vs 37.19%, p=0.008). The model overfits to training disease pairs and doesn't generalize.",
      "implication": "DEFINITIVE: No function of Node2Vec features can improve on cosine similarity for kNN. Both XGBoost classification (h5), XGBoost with kNN feature (h39 approach D), and now XGBoost regression on disease pairs ALL suffer from the same generalization failure. The DRKG-derived information is fully captured by cosine similarity. The 37% R@30 kNN baseline is the ceiling for DRKG-only approaches. Improvement requires EXTERNAL data sources."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h19",
      "learning": "HPO phenotype similarity adds NO value over Node2Vec for drug repurposing. Coverage limited to 13.3% of diseases, and Node2Vec outperforms HPO 2:1 on that subset.",
      "implication": "External phenotype data unlikely to help; Node2Vec already captures disease relationships. Future external data efforts should focus on drug/target-side enrichment, not disease phenotypes."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h19",
      "learning": "HPO phenotype similarity provides weaker signal than Node2Vec cosine (14.20% vs 36.91% R@30). Even on HPO-covered diseases, Node2Vec wins (62.3% vs 53.6%). Low correlation (0.126) suggests independent signals, but HPO is strictly inferior.",
      "implication": "External phenotype ontology data is NOT the path to breaking the 37% ceiling. Focus should shift to other external data sources (clinical trials, PPI networks) or fundamentally different approaches."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h17",
      "learning": "PPI distance is statistically informative (2.2x enrichment for GT pairs) but O(n\u00b2) computation makes direct use impractical. Would need precomputation or use as post-hoc filter.",
      "implication": "External network data CAN help but needs efficient integration strategy. Consider: (1) precompute drug-gene distances, (2) use as filter not ranking, (3) integrate into kNN as drug similarity."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h17",
      "learning": "PPI neighborhood similarity (16.18% R@30) is far worse than Node2Vec (36.93%). 2-hop neighborhoods average 4,828 genes \u2014 too large for meaningful similarity. DRKG already captures drug-gene-disease relationships.",
      "implication": "External PPI data is NOT the path to breaking the 37% ceiling. STRING data is already incorporated in DRKG structure."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h19,h17",
      "learning": "External data (HPO phenotype, PPI network) provides WEAKER signal than Node2Vec cosine (14-16% vs 37% R@30). The 37% ceiling is NOT due to missing external data \u2014 it's a fundamental limitation of similarity-based collaborative filtering.",
      "implication": "Breaking the ceiling requires (1) better GT coverage for kNN, (2) fundamentally different architectures (GNN, attention), or (3) hybrid approaches that don't rely on disease similarity."
    },
    {
      "date": "2026-01-28",
      "hypothesis_id": "h48",
      "learning": "kNN ceiling (37% R@30) is caused by GT coverage sparsity: 44.3% of test diseases have 0% drug overlap with k=20 nearest training diseases. Correlation(coverage, recall)=0.898. The algorithm is optimal for its coverage.",
      "implication": "Improvement requires (1) more GT data, (2) drug-centric approach for zero-coverage diseases, or (3) zero-shot methods. External similarity measures (HPO, PPI) failed because they don't address coverage \u2014 they just provide alternative similarity rankings for the same sparse GT."
    },
    {
      "date": "2026-01-29",
      "hypothesis_id": "h47",
      "learning": "Zero-coverage diseases (42% of test diseases) only contain 15% of test GT pairs. Zero-shot approaches have limited impact ceiling (+1.5 pp max for 10% recall).",
      "implication": "Focus on improving kNN for the 85% of GT pairs with coverage rather than zero-shot approaches"
    },
    {
      "date": "2026-01-29",
      "hypothesis_id": "h46",
      "learning": "Drug-centric repurposing is ill-defined for disease-holdout evaluation. Two-hop drug-disease-drug similarity is already captured by Node2Vec embeddings.",
      "implication": "For disease-holdout evaluation, disease-centric approaches (kNN) are the correct framing"
    },
    {
      "date": "2026-01-29",
      "hypothesis_id": "h13",
      "learning": "Confounding pattern expansion added 60+ patterns but validation cache doesn't contain the target biologics. Detection rate unchanged (1.43%) but coverage improved for future predictions.",
      "implication": "Measure confounding detection on raw model predictions, not curated validation cache"
    }
  ]
}